{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgJk8qEvaNGgp7lOb/0xt6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivasvarma123/Data-Engineering-concepts/blob/main/Joins_and_aggregations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XguhSxf21DqN"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"pyspark\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data4 = [\n",
        "    (1, \"raj\"),\n",
        "    (2, \"ravi\"),\n",
        "    (3, \"sai\"),\n",
        "    (5, \"rani\")\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data4, [\"id\", \"name\"])\n",
        "df1.show()\n",
        "\n",
        "data3 = [\n",
        "    (1, \"mouse\"),\n",
        "    (3, \"mobile\"),\n",
        "    (7, \"laptop\")\n",
        "]\n",
        "\n",
        "df2 = spark.createDataFrame(data3, [\"id\", \"product\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EIbjmno1JGb",
        "outputId": "333195b5-2764-4f31-ce6e-b402375c2324"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  2|ravi|\n",
            "|  3| sai|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "+---+-------+\n",
            "| id|product|\n",
            "+---+-------+\n",
            "|  1|  mouse|\n",
            "|  3| mobile|\n",
            "|  7| laptop|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INNER JOIN\n",
        "innerjoin = df1.join(df2, [\"id\"], \"inner\")\n",
        "innerjoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGmSBb7v1Ohz",
        "outputId": "8dfcd197-4321-4dd8-bf22-0ba8567eb3da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  3| sai| mobile|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LEFT JOIN\n",
        "leftjoin = df1.join(df2, [\"id\"], \"left\").orderBy(\"id\")\n",
        "leftjoin.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDJkgH6V1OvQ",
        "outputId": "e4cd4c13-ebbd-49af-ffb4-e0614c0b0be5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  2|ravi|   NULL|\n",
            "|  3| sai| mobile|\n",
            "|  5|rani|   NULL|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RIGHT JOIN\n",
        "rightjoin = df1.join(df2, [\"id\"], \"right\").orderBy(\"id\")\n",
        "rightjoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdByM4Xx1Oy3",
        "outputId": "23f0fd5f-cfe6-4b45-e603-ce2a11212676"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  3| sai| mobile|\n",
            "|  7|NULL| laptop|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FULL JOIN\n",
        "fulljoin = df1.join(df2, [\"id\"], \"full\").orderBy(\"id\")\n",
        "fulljoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myGBVpuQ1O3E",
        "outputId": "cd810036-62c3-48b7-f3a7-52a185e90a21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  2|ravi|   NULL|\n",
            "|  3| sai| mobile|\n",
            "|  5|rani|   NULL|\n",
            "|  7|NULL| laptop|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WHAT IF THE JOINING COLUMNS ARE DIFFERENTLY NAMED\n",
        "data4 = [\n",
        "    (1, \"raj\"),\n",
        "    (2, \"ravi\"),\n",
        "    (3, \"sai\"),\n",
        "    (5, \"rani\")\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data4, [\"id\", \"name\"])\n",
        "df1.show()\n",
        "\n",
        "data3 = [\n",
        "    (1, \"mouse\"),\n",
        "    (3, \"mobile\"),\n",
        "    (7, \"laptop\")\n",
        "]\n",
        "\n",
        "df2 = spark.createDataFrame(data3, [\"id1\", \"product\"])\n",
        "df2.show()\n",
        "\n",
        "innerjoin = df1.join(df2, df1[\"id\"] == df2[\"id1\"], \"inner\")\n",
        "innerjoin.show()\n",
        "innerjoin = df1.join(df2, df1[\"id\"] == df2[\"id1\"], \"inner\").drop(\"id1\")\n",
        "innerjoin.show()\n",
        "\n",
        "leftjoin = df1.join(df2, df1[\"id\"] == df2[\"id1\"], \"left\").orderBy(\"id\").drop(\"id1\")\n",
        "leftjoin.show()\n",
        "\n",
        "rightjoin = df1.join(df2, df1[\"id\"] == df2[\"id1\"], \"right\").orderBy(\"id1\").drop(\"id\")\n",
        "rightjoin.show()\n",
        "\n",
        "fulljoin  = (\n",
        "            df1.join(df2, df1[\"id\"] == df2[\"id1\"], \"full\")\n",
        "            .withColumn(\"id\", expr(\"CASE WHEN id IS NULL THEN id1 ELSE id END\"))\n",
        "            .orderBy(\"id\")\n",
        "            .drop(\"id1\")\n",
        "            )\n",
        "fulljoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0J8zqw11UuJ",
        "outputId": "9fc74577-789a-4cf1-abaa-26dc1fe15939"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  2|ravi|\n",
            "|  3| sai|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "+---+-------+\n",
            "|id1|product|\n",
            "+---+-------+\n",
            "|  1|  mouse|\n",
            "|  3| mobile|\n",
            "|  7| laptop|\n",
            "+---+-------+\n",
            "\n",
            "+---+----+---+-------+\n",
            "| id|name|id1|product|\n",
            "+---+----+---+-------+\n",
            "|  1| raj|  1|  mouse|\n",
            "|  3| sai|  3| mobile|\n",
            "+---+----+---+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  3| sai| mobile|\n",
            "+---+----+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  2|ravi|   NULL|\n",
            "|  3| sai| mobile|\n",
            "|  5|rani|   NULL|\n",
            "+---+----+-------+\n",
            "\n",
            "+----+---+-------+\n",
            "|name|id1|product|\n",
            "+----+---+-------+\n",
            "| raj|  1|  mouse|\n",
            "| sai|  3| mobile|\n",
            "|NULL|  7| laptop|\n",
            "+----+---+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  2|ravi|   NULL|\n",
            "|  3| sai| mobile|\n",
            "|  5|rani|   NULL|\n",
            "|  7|NULL| laptop|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SCENARIO 1\n",
        "source_rdd = spark.sparkContext.parallelize([\n",
        "    (1, \"A\"),\n",
        "    (2, \"B\"),\n",
        "    (3, \"C\"),\n",
        "    (4, \"D\")\n",
        "],1)\n",
        "\n",
        "target_rdd = spark.sparkContext.parallelize([\n",
        "    (1, \"A\"),\n",
        "    (2, \"B\"),\n",
        "    (4, \"X\"),\n",
        "    (5, \"F\")\n",
        "],2)\n",
        "\n",
        "\n",
        "# Convert RDDs to DataFrames using toDF()\n",
        "df1 = source_rdd.toDF([\"id\", \"name\"])\n",
        "df2 = target_rdd.toDF([\"id\",\"name1\"])\n",
        "\n",
        "# Show the DataFrames\n",
        "df1.show()\n",
        "df2.show()\n",
        "\n",
        "\n",
        "print(\"===== FULL JOIN=====\")\n",
        "fulljoin = df1.join(df2, [\"id\"], \"full\")\n",
        "fulljoin.show()\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "print(\"=====NAME AND NAME 1 MATCH=====\")\n",
        "procdf = fulljoin.withColumn(\"status\", expr(\"CASE WHEN name == name1 THEN 'match' ELSE 'mismatch' END\"))\n",
        "procdf.show()\n",
        "\n",
        "print(\"=====FILTER MISMATCH=====\")\n",
        "fildf = procdf.filter(\"status = 'mismatch'\")\n",
        "fildf.show()\n",
        "\n",
        "print(\"=====NULL CHECKS=====\")\n",
        "procdf1 = (\n",
        "            fildf.withColumn(\"status\",expr(\"\"\"\n",
        "                                CASE\n",
        "                                WHEN name1 IS NULL THEN 'New in Source'\n",
        "                                WHEN name IS NULL THEN 'New in Target'\n",
        "                                ELSE status\n",
        "                                END\n",
        "            \"\"\"))\n",
        ")\n",
        "procdf1.show()\n",
        "\n",
        "\n",
        "print(\"=====FINAL PROC=====\")\n",
        "finaldf = procdf1.drop(\"name\", \"name1\").withColumnRenamed(\"status\", \"comment\")\n",
        "finaldf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8QxhqBW1Uxt",
        "outputId": "25f0e711-110c-4f01-b64c-c55e46706a2c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|   A|\n",
            "|  2|   B|\n",
            "|  3|   C|\n",
            "|  4|   D|\n",
            "+---+----+\n",
            "\n",
            "+---+-----+\n",
            "| id|name1|\n",
            "+---+-----+\n",
            "|  1|    A|\n",
            "|  2|    B|\n",
            "|  4|    X|\n",
            "|  5|    F|\n",
            "+---+-----+\n",
            "\n",
            "===== FULL JOIN=====\n",
            "+---+----+-----+\n",
            "| id|name|name1|\n",
            "+---+----+-----+\n",
            "|  1|   A|    A|\n",
            "|  2|   B|    B|\n",
            "|  3|   C| NULL|\n",
            "|  4|   D|    X|\n",
            "|  5|NULL|    F|\n",
            "+---+----+-----+\n",
            "\n",
            "=====NAME AND NAME 1 MATCH=====\n",
            "+---+----+-----+--------+\n",
            "| id|name|name1|  status|\n",
            "+---+----+-----+--------+\n",
            "|  1|   A|    A|   match|\n",
            "|  2|   B|    B|   match|\n",
            "|  3|   C| NULL|mismatch|\n",
            "|  4|   D|    X|mismatch|\n",
            "|  5|NULL|    F|mismatch|\n",
            "+---+----+-----+--------+\n",
            "\n",
            "=====FILTER MISMATCH=====\n",
            "+---+----+-----+--------+\n",
            "| id|name|name1|  status|\n",
            "+---+----+-----+--------+\n",
            "|  3|   C| NULL|mismatch|\n",
            "|  4|   D|    X|mismatch|\n",
            "|  5|NULL|    F|mismatch|\n",
            "+---+----+-----+--------+\n",
            "\n",
            "=====NULL CHECKS=====\n",
            "+---+----+-----+-------------+\n",
            "| id|name|name1|       status|\n",
            "+---+----+-----+-------------+\n",
            "|  3|   C| NULL|New in Source|\n",
            "|  4|   D|    X|     mismatch|\n",
            "|  5|NULL|    F|New in Target|\n",
            "+---+----+-----+-------------+\n",
            "\n",
            "=====FINAL PROC=====\n",
            "+---+-------------+\n",
            "| id|      comment|\n",
            "+---+-------------+\n",
            "|  3|New in Source|\n",
            "|  4|     mismatch|\n",
            "|  5|New in Target|\n",
            "+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,\"Veg Biryani\"),(2,\"Veg Fried Rice\"),(3,\"Kaju Fried Rice\"),(4,\"Chicken Biryani\"),(5,\"Chicken Dum Biryani\"),(6,\"Prawns Biryani\"),(7,\"Fish Birayani\")]\n",
        "\n",
        "df1 = spark.createDataFrame(data,[\"food_id\",\"food_item\"])\n",
        "df1.show()\n",
        "\n",
        "ratings = [(1,5),(2,3),(3,4),(4,4),(5,5),(6,4),(7,4)]\n",
        "\n",
        "df2 = spark.createDataFrame(ratings,[\"food_id\",\"rating\"])\n",
        "df2.show()\n",
        "\n",
        "leftjoin = df1.join(df2, \"food_id\", \"left\").orderBy(\"food_id\").withColumn(\"stats(Out of 5)\", expr(\"repeat('*',rating)\"))\n",
        "leftjoin.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7NR10Ua1U1G",
        "outputId": "888ff65e-cebb-490b-9f4f-b2c403ec8b54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|food_id|          food_item|\n",
            "+-------+-------------------+\n",
            "|      1|        Veg Biryani|\n",
            "|      2|     Veg Fried Rice|\n",
            "|      3|    Kaju Fried Rice|\n",
            "|      4|    Chicken Biryani|\n",
            "|      5|Chicken Dum Biryani|\n",
            "|      6|     Prawns Biryani|\n",
            "|      7|      Fish Birayani|\n",
            "+-------+-------------------+\n",
            "\n",
            "+-------+------+\n",
            "|food_id|rating|\n",
            "+-------+------+\n",
            "|      1|     5|\n",
            "|      2|     3|\n",
            "|      3|     4|\n",
            "|      4|     4|\n",
            "|      5|     5|\n",
            "|      6|     4|\n",
            "|      7|     4|\n",
            "+-------+------+\n",
            "\n",
            "+-------+-------------------+------+---------------+\n",
            "|food_id|          food_item|rating|stats(Out of 5)|\n",
            "+-------+-------------------+------+---------------+\n",
            "|      1|        Veg Biryani|     5|          *****|\n",
            "|      2|     Veg Fried Rice|     3|            ***|\n",
            "|      3|    Kaju Fried Rice|     4|           ****|\n",
            "|      4|    Chicken Biryani|     4|           ****|\n",
            "|      5|Chicken Dum Biryani|     5|          *****|\n",
            "|      6|     Prawns Biryani|     4|           ****|\n",
            "|      7|      Fish Birayani|     4|           ****|\n",
            "+-------+-------------------+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ANTI JOIN\n",
        "data4 = [\n",
        "    (1, \"raj\"),\n",
        "    (2, \"ravi\"),\n",
        "    (3, \"sai\"),\n",
        "    (5, \"rani\")\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data4, [\"id\", \"name\"])\n",
        "df1.show()\n",
        "\n",
        "data3 = [\n",
        "    (1, \"mouse\"),\n",
        "    (3, \"mobile\"),\n",
        "    (7, \"laptop\")\n",
        "]\n",
        "\n",
        "df2 = spark.createDataFrame(data3, [\"id\", \"product\"])\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-BeWa7z1U4e",
        "outputId": "6515cd53-fa7f-4f9f-d2e6-bd6f1f33cfc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  2|ravi|\n",
            "|  3| sai|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "+---+-------+\n",
            "| id|product|\n",
            "+---+-------+\n",
            "|  1|  mouse|\n",
            "|  3| mobile|\n",
            "|  7| laptop|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WRONG METHOD WHICH WILL LEAD TO SKEWNESS\n",
        "listval = df2.select(\"id\").rdd.flatMap(lambda x:x).collect()\n",
        "print(listval)\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "finaldf = df1.filter(~F.col('id').isin(listval))\n",
        "finaldf.show()\n",
        "\n",
        "#CORRECT METHOD\n",
        "antijoin = df1.join(df2, \"id\", \"left_anti\")\n",
        "antijoin.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8PxOhbJ1U7q",
        "outputId": "0f94aaf2-d185-4264-b36b-f350d1111e21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 7]\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  2|ravi|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  2|ravi|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CROSS JOIN\n",
        "crossjoin = df1.crossJoin(df2.withColumnRenamed(\"id\",\"id1\"))\n",
        "crossjoin.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFou215V1VCA",
        "outputId": "1eb00d81-2f99-4bd6-d902-0ad78f30ef75"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+-------+\n",
            "| id|name|id1|product|\n",
            "+---+----+---+-------+\n",
            "|  1| raj|  1|  mouse|\n",
            "|  2|ravi|  1|  mouse|\n",
            "|  1| raj|  3| mobile|\n",
            "|  1| raj|  7| laptop|\n",
            "|  2|ravi|  3| mobile|\n",
            "|  2|ravi|  7| laptop|\n",
            "|  3| sai|  1|  mouse|\n",
            "|  5|rani|  1|  mouse|\n",
            "|  3| sai|  3| mobile|\n",
            "|  3| sai|  7| laptop|\n",
            "|  5|rani|  3| mobile|\n",
            "|  5|rani|  7| laptop|\n",
            "+---+----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", \"AA\"), (\"B\", \"BB\"), (\"C\", \"CC\"), (\"AA\", \"AAA\"), (\"BB\", \"BBB\"), (\"CC\", \"CCC\")]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"child\", \"parent\"])\n",
        "df.show()\n",
        "\n",
        "df1 = df\n",
        "df2 = df.withColumnRenamed(\"child\",\"child1\").withColumnRenamed(\"parent\",\"parent1\")\n",
        "\n",
        "df1.show()\n",
        "df2.show()\n",
        "\n",
        "joindf = df1.join(df2, df1[\"child\"] ==  df2[\"parent1\"])\n",
        "joindf.show()\n",
        "\n",
        "finaldf = (\n",
        "            joindf\n",
        "            .drop(\"parent1\")\n",
        "            .withColumnRenamed(\"child\",\"parent1\")\n",
        "            .withColumnRenamed(\"parent\",\"grand parent\")\n",
        "            .withColumnRenamed(\"child1\",\"child\")\n",
        "            .withColumnRenamed(\"parent1\",\"parent\")\n",
        "            .select(\"child\",\"parent\",\"grand parent\")\n",
        ")\n",
        "finaldf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUDypdKm1O8N",
        "outputId": "de751da0-9afd-4b69-96c9-a397d22abd91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|child|parent|\n",
            "+-----+------+\n",
            "|    A|    AA|\n",
            "|    B|    BB|\n",
            "|    C|    CC|\n",
            "|   AA|   AAA|\n",
            "|   BB|   BBB|\n",
            "|   CC|   CCC|\n",
            "+-----+------+\n",
            "\n",
            "+-----+------+\n",
            "|child|parent|\n",
            "+-----+------+\n",
            "|    A|    AA|\n",
            "|    B|    BB|\n",
            "|    C|    CC|\n",
            "|   AA|   AAA|\n",
            "|   BB|   BBB|\n",
            "|   CC|   CCC|\n",
            "+-----+------+\n",
            "\n",
            "+------+-------+\n",
            "|child1|parent1|\n",
            "+------+-------+\n",
            "|     A|     AA|\n",
            "|     B|     BB|\n",
            "|     C|     CC|\n",
            "|    AA|    AAA|\n",
            "|    BB|    BBB|\n",
            "|    CC|    CCC|\n",
            "+------+-------+\n",
            "\n",
            "+-----+------+------+-------+\n",
            "|child|parent|child1|parent1|\n",
            "+-----+------+------+-------+\n",
            "|   AA|   AAA|     A|     AA|\n",
            "|   BB|   BBB|     B|     BB|\n",
            "|   CC|   CCC|     C|     CC|\n",
            "+-----+------+------+-------+\n",
            "\n",
            "+-----+------+------------+\n",
            "|child|parent|grand parent|\n",
            "+-----+------+------------+\n",
            "|    A|    AA|         AAA|\n",
            "|    B|    BB|         BBB|\n",
            "|    C|    CC|         CCC|\n",
            "+-----+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AGGREGATION\n",
        "data = [(\"sai\", 40), (\"zeyo\", 30), (\"sai\", 50), (\"zeyo\", 40), (\"sai\", 10)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"amount\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "aggdf = (\n",
        "        df\n",
        "        .groupBy(\"name\")\n",
        "        .agg(sum(\"amount\").alias(\"total\"), count(\"name\").alias(\"cnt\"))\n",
        ")\n",
        "aggdf.show()\n",
        "\n",
        "\n",
        "data1 = [(\"sai\",\"chennai\", 40), (\"sai\",\"hydb\", 50), (\"sai\",\"chennai\", 10), (\"sai\",\"hydb\", 60)]\n",
        "df1 = spark.createDataFrame(data1, [\"name\", \"location\", \"amount\"])\n",
        "df1.show()\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "aggdf1 = (\n",
        "            df1\n",
        "            .groupBy(\"name\",\"location\")\n",
        "            .agg(sum(\"amount\").alias(\"total\"))\n",
        ")\n",
        "aggdf1.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRhQJ7Tp2lZ_",
        "outputId": "d8e77169-d6e5-4750-c339-b97b42656bfe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|name|amount|\n",
            "+----+------+\n",
            "| sai|    40|\n",
            "|zeyo|    30|\n",
            "| sai|    50|\n",
            "|zeyo|    40|\n",
            "| sai|    10|\n",
            "+----+------+\n",
            "\n",
            "+----+-----+---+\n",
            "|name|total|cnt|\n",
            "+----+-----+---+\n",
            "| sai|  100|  3|\n",
            "|zeyo|   70|  2|\n",
            "+----+-----+---+\n",
            "\n",
            "+----+--------+------+\n",
            "|name|location|amount|\n",
            "+----+--------+------+\n",
            "| sai| chennai|    40|\n",
            "| sai|    hydb|    50|\n",
            "| sai| chennai|    10|\n",
            "| sai|    hydb|    60|\n",
            "+----+--------+------+\n",
            "\n",
            "+----+--------+-----+\n",
            "|name|location|total|\n",
            "+----+--------+-----+\n",
            "| sai| chennai|   50|\n",
            "| sai|    hydb|  110|\n",
            "+----+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 1\n",
        "data1 = [\n",
        "    (1, \"A\", \"A\", 1000000),\n",
        "    (2, \"B\", \"A\", 2500000),\n",
        "    (3, \"C\", \"G\", 500000),\n",
        "    (4, \"D\", \"G\", 800000),\n",
        "    (5, \"E\", \"W\", 9000000),\n",
        "    (6, \"F\", \"W\", 2000000),\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, [\"emp_id\",\"name\",\"dept_id\",\"salary\"])\n",
        "df1.show()\n",
        "\n",
        "data2 = [(\"A\", \"AZURE\"), (\"G\", \"GCP\"), (\"W\", \"AWS\")]\n",
        "df2 = spark.createDataFrame(data2, [\"dept_id1\", \"dept_name\"])\n",
        "df2.show()\n",
        "\n",
        "joindf = df1.join(df2, df1[\"dept_id\"] == df2[\"dept_id1\"], \"left\" )\n",
        "joindf.show()\n",
        "\n",
        "seldf = joindf.select(\"emp_id\",\"name\",\"dept_name\",\"salary\").orderBy(\"dept_name\",\"salary\")\n",
        "seldf.show()\n",
        "\n",
        "finaldf = (\n",
        "            seldf\n",
        "            .groupBy(\"dept_name\")\n",
        "            .agg(min(\"salary\").alias(\"salary\"))\n",
        ")\n",
        "finaldf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSrjIUg22zSh",
        "outputId": "fe0f421d-74fb-4135-ece6-d0fb028c7d91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-------+-------+\n",
            "|emp_id|name|dept_id| salary|\n",
            "+------+----+-------+-------+\n",
            "|     1|   A|      A|1000000|\n",
            "|     2|   B|      A|2500000|\n",
            "|     3|   C|      G| 500000|\n",
            "|     4|   D|      G| 800000|\n",
            "|     5|   E|      W|9000000|\n",
            "|     6|   F|      W|2000000|\n",
            "+------+----+-------+-------+\n",
            "\n",
            "+--------+---------+\n",
            "|dept_id1|dept_name|\n",
            "+--------+---------+\n",
            "|       A|    AZURE|\n",
            "|       G|      GCP|\n",
            "|       W|      AWS|\n",
            "+--------+---------+\n",
            "\n",
            "+------+----+-------+-------+--------+---------+\n",
            "|emp_id|name|dept_id| salary|dept_id1|dept_name|\n",
            "+------+----+-------+-------+--------+---------+\n",
            "|     1|   A|      A|1000000|       A|    AZURE|\n",
            "|     2|   B|      A|2500000|       A|    AZURE|\n",
            "|     3|   C|      G| 500000|       G|      GCP|\n",
            "|     5|   E|      W|9000000|       W|      AWS|\n",
            "|     6|   F|      W|2000000|       W|      AWS|\n",
            "|     4|   D|      G| 800000|       G|      GCP|\n",
            "+------+----+-------+-------+--------+---------+\n",
            "\n",
            "+------+----+---------+-------+\n",
            "|emp_id|name|dept_name| salary|\n",
            "+------+----+---------+-------+\n",
            "|     6|   F|      AWS|2000000|\n",
            "|     5|   E|      AWS|9000000|\n",
            "|     1|   A|    AZURE|1000000|\n",
            "|     2|   B|    AZURE|2500000|\n",
            "|     3|   C|      GCP| 500000|\n",
            "|     4|   D|      GCP| 800000|\n",
            "+------+----+---------+-------+\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name| salary|\n",
            "+---------+-------+\n",
            "|      AWS|2000000|\n",
            "|      GCP| 500000|\n",
            "|    AZURE|1000000|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}